{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Generative Systems (SGS) with an integrated Large Language Model (LLM). \n",
    "\n",
    "This presentation outlines an efficient and unobtrusive method for integrating AI models, specifically Large Language Models (LLMs), into the SGS framework in a plug-and-play manner. A central aspect of this integration is the shared tokenization process, which supplies input to both the Metadata Models (MM) and the LLMs. This shared input guarantees that both components process the same foundational data, thereby enhancing system performance through improved efficiency and coherence.\n",
    "\n",
    "The solution focuses on enhancing the tokenizer of the specific LLM, which in our case is the GPT2Tokenizer.\n",
    "\n",
    "Within the ExtendedGPT2Tokenizer, two key methods are responsible for these tasks:\n",
    "\n",
    "- `tokenize_text(text)`: This method tokenizes the input text.\n",
    "- `update_tensors(token_ids)`: This method updates the tensors, which hold Meta Model HllSets data, based on the token IDs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor_to_array (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using PyCall\n",
    "using DataFrames\n",
    "\n",
    "# Import the fine_tune_model and parse_decoded_strings functions from the Python script\n",
    "py\"\"\"\n",
    "import sys\n",
    "sys.path.append(\".\")\n",
    "from SGS_Tokenizer import ExtendedGPT2Tokenizer, GPT2Tokenizer, GPT2LMHeadModel\n",
    "\"\"\"\n",
    "\n",
    "function tensor_to_array(tensor::PyObject)\n",
    "    # Convert PyObject to Julia Vector{UInt32}\n",
    "    hllset_array = pycall(tensor.numpy, PyArray)\n",
    "    hllset_vector = Vector{Int64}(hllset_array)\n",
    "\n",
    "    return hllset_vector\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Update Process\n",
    "\n",
    "1. Obtain token IDs:\n",
    "\n",
    "```python\n",
    "token_ids = tokenizer.tokenize_text(text)\n",
    "```\n",
    "\n",
    "2. Update tensors:\n",
    "\n",
    "```python\n",
    "new_tensor_1, double_value = tokenizer.update_tensors(token_ids)\n",
    "```\n",
    "\n",
    "This streamlined approach ensures that the integration of AI models into the SGS framework is both seamless and effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HLLSet:1; 4da9de3e80bcb65ee6169a411b0206ce45ba68bc; PyObject tensor([ 2, 18, 12,  2,  2, 28,  0,  8,  0, 22,  0,  0,  0,  2,  0,  2])\n",
      "Tensor Slice:PyObject tensor([[1.0220e+03, 4.0546e+07, 1.0000e-01],\n",
      "        [1.2000e+01, 1.6594e+08, 1.0000e-01],\n",
      "        [7.3400e+02, 3.3153e+08, 1.1000e+00],\n",
      "        [3.0104e+04, 5.2051e+08, 1.1000e+00],\n",
      "        [5.1100e+02, 4.0169e+08, 1.1000e+00],\n",
      "        [2.6200e+02, 3.9209e+08, 1.4000e+00],\n",
      "        [2.6200e+02, 3.9209e+08, 1.4000e+00],\n",
      "        [3.1800e+02, 5.7578e+08, 2.2000e+00],\n",
      "        [3.5800e+03, 6.3241e+08, 2.3000e+00],\n",
      "        [7.8800e+02, 9.3641e+08, 3.1000e+00],\n",
      "        [4.1290e+03, 1.1767e+09, 4.1000e+00],\n",
      "        [2.8400e+02, 1.3680e+09, 5.2000e+00],\n",
      "        [4.3260e+03, 1.4954e+09, 5.3000e+00],\n",
      "        [1.5879e+04, 1.5664e+09, 5.4000e+00],\n",
      "        [3.0700e+02, 1.8849e+09, 7.3000e+00],\n",
      "        [5.2530e+03, 2.4650e+09, 9.1000e+00],\n",
      "        [1.3664e+04, 2.6029e+09, 9.2000e+00],\n",
      "        [5.4470e+03, 2.5253e+09, 9.4000e+00],\n",
      "        [2.2150e+03, 3.5021e+09, 1.3100e+01],\n",
      "        [2.8600e+02, 4.2398e+09, 1.5100e+01]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "text = \"When the distance between two unit-length vectors is defined to be the length of their vector difference then\"\n",
    "\n",
    "vocab_file = \"JLD2/vocab.json\"      # Path to the vocab file\n",
    "merges_file = \"JLD2/merges.txt\"     # Path to the merges file\n",
    "\n",
    "tokenizer = py\"ExtendedGPT2Tokenizer\"(vocab_file, merges_file, p=4)\n",
    "\n",
    "# text = \"When the distance between two unit-length vectors is defined to be the length of their vector difference then\"\n",
    "\n",
    "# Update tensors\n",
    "token_ids = tokenizer.tokenize_text(text)\n",
    "new_tensor_1, double_value = tokenizer.update_tensors(token_ids)\n",
    "\n",
    "# println(\"new_tensor_1:\", new_tensor_1)\n",
    "# println(\"double_value:\", double_value)\n",
    "\n",
    "id, sha1, hll_tensor = tokenizer.tensor_to_hlltensor(new_tensor_1)\n",
    "println(\"HLLSet:\", id, \"; \", sha1, \"; \", hll_tensor)\n",
    "\n",
    "hll_vector = tensor_to_array(hll_tensor)\n",
    "\n",
    "# println(\"HLLSet (Vector{Int64}):\", hll_vector)\n",
    "\n",
    "tensor_slice = tokenizer.hlltensor_to_tensor(hll_tensor)\n",
    "println(\"Tensor Slice:\", tensor_slice)\n",
    "\n",
    "# tokenizer.print_tensor_1(tensor=tokenizer.tensor_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.print_tensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, we have successfully integrated text into both the Large Language Model (LLM) and the Metadata Model (MM), making the tokens accessible for search and retrieval from both models.\n",
    "\n",
    "To retrieve all HllSets related to the query from the MM, we follow these steps:\n",
    "\n",
    "1. We create an HllSet based on the query text, utilizing the same tokenization process we employed previously.\n",
    "2. We then search for all HllSets in the MM that are similar to the query HllSet, using cosine similarity to identify those that meet a specified threshold.\n",
    "3. Finally, leveraging the token hash-to-token ID mapping, we query the LLM with the obtained token IDs. The resulting tokens will represent a collection of related tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Related HLL sets: PyObject [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PyObject [' between', '-', ' two', ' vectors', ' their', ' the', ' the', ' is', ' difference', ' then', ' length', ' to', ' unit', ' vector', ' be', ' distance', 'length', ' defined', 'When', ' of']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Perform search\n",
    "query = \"When the distance between two unit-length vectors is defined\"\n",
    "\n",
    "threshold = 0.1\n",
    "related_hllsets = pycall(tokenizer.search, PyObject, query, threshold)\n",
    "println(\"Related HLL sets: \", related_hllsets)\n",
    "\n",
    "# Get related tokens\n",
    "related_tokens = pycall(tokenizer.get_related_tokens, PyObject, related_hllsets)\n",
    "# println(\"Related tokens: \", related_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now request suggestions from the LLM that align with the extracted related tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated text suggestions:\n",
      "Suggestion 1:\n",
      "-lengthWhenThis article is about a character, it is not about them. For other uses of the term \"character\", see Character (disambiguation).\n",
      "\n",
      "Suggestion 2:\n",
      "-lengthWhenThis article is about a character, it is not about them. For other uses of the term \"character\", see Character (disambiguation)\n",
      "\n",
      "Suggestion 3:\n",
      "-lengthWhenThis article is about a character, it is not about them. For other uses of the term \"character\", see Character.\n",
      "\n",
      "\"I'm\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate meaningful text\n",
    "suggestions = []\n",
    "try    \n",
    "    suggestions = pycall(tokenizer.generate_text, PyObject, related_tokens, 3)\n",
    "catch e\n",
    "    println(\"Error generating text suggestions: \", e)\n",
    "end\n",
    "\n",
    "\n",
    "println(tokenizer.format_generated_texts(suggestions))\n",
    "\n",
    "# tokenizer.print_tensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that the HllSets in the Meta Model are organized in some manner. One common approach to this organization is to group HllSets into related communities using techniques like cosine similarity. This allows us to evaluate the generated suggestions and select the most relevant ones based on our interests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: \n",
      "Generated text suggestions:\n",
      "Suggestion 1:\n",
      "('-lengthWhenThis article is about a character, it is not about them. For other uses of the term \"character\", see Character (disambiguation).', '4da9de3e80bcb65ee6169a411b0206ce45ba68bc', np.float64(0.21087891921820445))\n",
      "\n",
      "Suggestion 2:\n",
      "('-lengthWhenThis article is about a character, it is not about them. For other uses of the term \"character\", see Character (disambiguation)', '4da9de3e80bcb65ee6169a411b0206ce45ba68bc', np.float64(0.2988664113443373))\n",
      "\n",
      "Suggestion 3:\n",
      "('-lengthWhenThis article is about a character, it is not about them. For other uses of the term \"character\", see Character.\\n\\n\"I\\'m', '4da9de3e80bcb65ee6169a411b0206ce45ba68bc', np.float64(0.29280576695787364))\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate generated texts\n",
    "communities = [\n",
    "    \n",
    "    \"4da9de3e80bcb65ee6169a411b0206ce45ba68bc\"\n",
    "    ]  # Load or define your communities of HLL sets here\n",
    "\n",
    "evaluation_results = pycall(tokenizer.evaluate_generated_texts, PyObject, suggestions, communities)\n",
    "println(\"Evaluation results: \", tokenizer.format_generated_texts(evaluation_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
