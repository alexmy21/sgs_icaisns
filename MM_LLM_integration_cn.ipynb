{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Generative Systems (SGS) with an integrated Large Language Model (LLM). \n",
    "\n",
    "# 集成大型语言模型（LLM）的自生成系统（SGS）\n",
    "\n",
    "本次演示介绍了一种高效且不显眼的方法，将AI模型，特别是大型语言模型（LLMs），以即插即用的方式集成到SGS框架中。此集成的一个关键特征是共享的分词过程，该过程为元数据模型（MM）和LLMs提供输入。这种共享输入确保两个组件处理相同的基础数据，从而通过保证效率和连贯性来提高系统性能。\n",
    "\n",
    "该解决方案涉及增强特定LLM的分词器——在我们的案例中是GPT2Tokenizer。\n",
    "\n",
    "在ExtendedGPT2Tokenizer中，有两个方法负责这些任务：\n",
    "\n",
    "- `tokenize_text(text)`: 该方法对输入文本进行分词。\n",
    "- `update_tensors(token_ids)`: 该方法根据令牌ID更新保存元模型HllSets数据的张量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor_to_array (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using PyCall\n",
    "using DataFrames\n",
    "\n",
    "# Import the fine_tune_model and parse_decoded_strings functions from the Python script\n",
    "py\"\"\"\n",
    "import sys\n",
    "sys.path.append(\".\")\n",
    "from SGS_Tokenizer import ExtendedGPT2Tokenizer, GPT2Tokenizer, GPT2LMHeadModel\n",
    "\"\"\"\n",
    "\n",
    "function tensor_to_array(tensor::PyObject)\n",
    "    # Convert PyObject to Julia Vector{UInt32}\n",
    "    hllset_array = pycall(tensor.numpy, PyArray)\n",
    "    hllset_vector = Vector{Int64}(hllset_array)\n",
    "\n",
    "    return hllset_vector\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 张量更新过程\n",
    "\n",
    "1. 获取令牌ID：\n",
    "```python\n",
    "token_ids = tokenizer.tokenize_text(text)\n",
    "```\n",
    "\n",
    "2. 更新张量：\n",
    "```python\n",
    "new_tensor_1, double_value = tokenizer.update_tensors(token_ids)\n",
    "```\n",
    "\n",
    "这种简化的方法确保AI模型在SGS框架中的集成既无缝又有效。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HLLSet:1; 4da9de3e80bcb65ee6169a411b0206ce45ba68bc; PyObject tensor([ 2, 18, 12,  2,  2, 28,  0,  8,  0, 22,  0,  0,  0,  2,  0,  2])\n",
      "Tensor Slice:PyObject tensor([[1.0220e+03, 4.0546e+07, 1.0000e-01],\n",
      "        [1.2000e+01, 1.6594e+08, 1.0000e-01],\n",
      "        [7.3400e+02, 3.3153e+08, 1.1000e+00],\n",
      "        [3.0104e+04, 5.2051e+08, 1.1000e+00],\n",
      "        [5.1100e+02, 4.0169e+08, 1.1000e+00],\n",
      "        [2.6200e+02, 3.9209e+08, 1.4000e+00],\n",
      "        [2.6200e+02, 3.9209e+08, 1.4000e+00],\n",
      "        [3.1800e+02, 5.7578e+08, 2.2000e+00],\n",
      "        [3.5800e+03, 6.3241e+08, 2.3000e+00],\n",
      "        [7.8800e+02, 9.3641e+08, 3.1000e+00],\n",
      "        [4.1290e+03, 1.1767e+09, 4.1000e+00],\n",
      "        [2.8400e+02, 1.3680e+09, 5.2000e+00],\n",
      "        [4.3260e+03, 1.4954e+09, 5.3000e+00],\n",
      "        [1.5879e+04, 1.5664e+09, 5.4000e+00],\n",
      "        [3.0700e+02, 1.8849e+09, 7.3000e+00],\n",
      "        [5.2530e+03, 2.4650e+09, 9.1000e+00],\n",
      "        [1.3664e+04, 2.6029e+09, 9.2000e+00],\n",
      "        [5.4470e+03, 2.5253e+09, 9.4000e+00],\n",
      "        [2.2150e+03, 3.5021e+09, 1.3100e+01],\n",
      "        [2.8600e+02, 4.2398e+09, 1.5100e+01]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "text = \"When the distance between two unit-length vectors is defined to be the length of their vector difference then\"\n",
    "\n",
    "vocab_file = \"JLD2/vocab.json\"      # Path to the vocab file\n",
    "merges_file = \"JLD2/merges.txt\"     # Path to the merges file\n",
    "\n",
    "tokenizer = py\"ExtendedGPT2Tokenizer\"(vocab_file, merges_file, p=4)\n",
    "\n",
    "# text = \"When the distance between two unit-length vectors is defined to be the length of their vector difference then\"\n",
    "\n",
    "# Update tensors\n",
    "token_ids = tokenizer.tokenize_text(text)\n",
    "new_tensor_1, double_value = tokenizer.update_tensors(token_ids)\n",
    "\n",
    "# println(\"new_tensor_1:\", new_tensor_1)\n",
    "# println(\"double_value:\", double_value)\n",
    "\n",
    "id, sha1, hll_tensor = tokenizer.tensor_to_hlltensor(new_tensor_1)\n",
    "println(\"HLLSet:\", id, \"; \", sha1, \"; \", hll_tensor)\n",
    "\n",
    "hll_vector = tensor_to_array(hll_tensor)\n",
    "\n",
    "# println(\"HLLSet (Vector{Int64}):\", hll_vector)\n",
    "\n",
    "tensor_slice = tokenizer.hlltensor_to_tensor(hll_tensor)\n",
    "println(\"Tensor Slice:\", tensor_slice)\n",
    "\n",
    "# tokenizer.print_tensor_1(tensor=tokenizer.tensor_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.print_tensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个阶段，我们已经成功地将文本集成到大型语言模型（LLM）和元数据模型（MM）中，使得这两个模型中的令牌可以进行搜索和检索。\n",
    "\n",
    "为了从MM中检索与查询相关的所有HllSets，我们遵循以下步骤：\n",
    "\n",
    "1. 我们根据查询文本创建一个HllSet，使用我们之前采用的相同分词过程。\n",
    "2. 然后，我们在MM中搜索与查询HllSet相似的所有HllSets，使用余弦相似度来识别满足指定阈值的HllSets。\n",
    "3. 最后，利用令牌哈希到令牌ID的映射，我们用获取的令牌ID查询LLM。结果令牌将代表一组相关的令牌。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Related HLL sets: PyObject [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PyObject [' between', '-', ' two', ' vectors', ' their', ' the', ' the', ' is', ' difference', ' then', ' length', ' to', ' unit', ' vector', ' be', ' distance', 'length', ' defined', 'When', ' of']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Perform search\n",
    "query = \"When the distance between two unit-length vectors is defined\"\n",
    "\n",
    "threshold = 0.1\n",
    "related_hllsets = pycall(tokenizer.search, PyObject, query, threshold)\n",
    "println(\"Related HLL sets: \", related_hllsets)\n",
    "\n",
    "# Get related tokens\n",
    "related_tokens = pycall(tokenizer.get_related_tokens, PyObject, related_hllsets)\n",
    "# println(\"Related tokens: \", related_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们现在可以请求LLM提供与提取的相关令牌一致的建议。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated text suggestions:\n",
      "Suggestion 1:\n",
      "-lengthWhenThis article is about a character, it is not about them. For other uses of the term \"character\", see Character (disambiguation).\n",
      "\n",
      "Suggestion 2:\n",
      "-lengthWhenThis article is about a character, it is not about them. For other uses of the term \"character\", see Character (disambiguation)\n",
      "\n",
      "Suggestion 3:\n",
      "-lengthWhenThis article is about a character, it is not about them. For other uses of the term \"character\", see Character.\n",
      "\n",
      "\"I'm\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate meaningful text\n",
    "suggestions = []\n",
    "try    \n",
    "    suggestions = pycall(tokenizer.generate_text, PyObject, related_tokens, 3)\n",
    "catch e\n",
    "    println(\"Error generating text suggestions: \", e)\n",
    "end\n",
    "\n",
    "\n",
    "println(tokenizer.format_generated_texts(suggestions))\n",
    "\n",
    "# tokenizer.print_tensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们假设元模型中的HllSets以某种方式组织。一个常见的组织方法是使用余弦相似度等技术将HllSets分组到相关社区中。这使我们能够评估生成的建议，并根据我们的兴趣选择最相关的建议。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: \n",
      "Generated text suggestions:\n",
      "Suggestion 1:\n",
      "('-lengthWhenThis article is about a character, it is not about them. For other uses of the term \"character\", see Character (disambiguation).', '4da9de3e80bcb65ee6169a411b0206ce45ba68bc', np.float64(0.21087891921820445))\n",
      "\n",
      "Suggestion 2:\n",
      "('-lengthWhenThis article is about a character, it is not about them. For other uses of the term \"character\", see Character (disambiguation)', '4da9de3e80bcb65ee6169a411b0206ce45ba68bc', np.float64(0.2988664113443373))\n",
      "\n",
      "Suggestion 3:\n",
      "('-lengthWhenThis article is about a character, it is not about them. For other uses of the term \"character\", see Character.\\n\\n\"I\\'m', '4da9de3e80bcb65ee6169a411b0206ce45ba68bc', np.float64(0.29280576695787364))\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate generated texts\n",
    "communities = [\n",
    "    \n",
    "    \"4da9de3e80bcb65ee6169a411b0206ce45ba68bc\"\n",
    "    ]  # Load or define your communities of HLL sets here\n",
    "\n",
    "evaluation_results = pycall(tokenizer.evaluate_generated_texts, PyObject, suggestions, communities)\n",
    "println(\"Evaluation results: \", tokenizer.format_generated_texts(evaluation_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
