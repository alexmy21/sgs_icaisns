{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Generative Systems (SGS) with an integrated Large Language Model (LLM). \n",
    "\n",
    "# 集成大型语言模型（LLM）的自生成系统（SGS）\n",
    "\n",
    "本次演示介绍了一种高效且不显眼的方法，将AI模型，特别是大型语言模型（LLMs），以即插即用的方式集成到SGS框架中。此集成的一个关键特征是共享的分词过程，该过程为元数据模型（MM）和LLMs提供输入。这种共享输入确保两个组件处理相同的基础数据，从而通过保证效率和连贯性来提高系统性能。\n",
    "\n",
    "该解决方案涉及增强特定LLM的分词器——在我们的案例中是GPT2Tokenizer。\n",
    "\n",
    "在ExtendedGPT2Tokenizer中，有两个方法负责这些任务：\n",
    "\n",
    "- `tokenize_text(text)`: 该方法对输入文本进行分词。\n",
    "- `update_tensors(token_ids)`: 该方法根据令牌ID更新保存元模型HllSets数据的张量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using PyCall\n",
    "using DataFrames\n",
    "\n",
    "# Import the fine_tune_model and parse_decoded_strings functions from the Python script\n",
    "py\"\"\"\n",
    "import sys\n",
    "sys.path.append(\".\")\n",
    "from SGS_Tokenizer import ExtendedGPT2Tokenizer, GPT2Tokenizer, GPT2LMHeadModel\n",
    "\"\"\"\n",
    "\n",
    "function tensor_to_array(tensor::PyObject)\n",
    "    # Convert PyObject to Julia Vector{UInt32}\n",
    "    hllset_array = pycall(tensor.numpy, PyArray)\n",
    "    hllset_vector = Vector{Int64}(hllset_array)\n",
    "\n",
    "    return hllset_vector\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 张量更新过程\n",
    "\n",
    "1. 获取令牌ID：\n",
    "```python\n",
    "token_ids = tokenizer.tokenize_text(text)\n",
    "```\n",
    "\n",
    "2. 更新张量：\n",
    "```python\n",
    "new_tensor_1, double_value = tokenizer.update_tensors(token_ids)\n",
    "```\n",
    "\n",
    "这种简化的方法确保AI模型在SGS框架中的集成既无缝又有效。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"When the distance between two unit-length vectors is defined to be the length of their vector difference then\"\n",
    "\n",
    "vocab_file = \"JLD2/vocab.json\"      # Path to the vocab file\n",
    "merges_file = \"JLD2/merges.txt\"     # Path to the merges file\n",
    "\n",
    "tokenizer = py\"ExtendedGPT2Tokenizer\"(vocab_file, merges_file, p=4)\n",
    "\n",
    "# text = \"When the distance between two unit-length vectors is defined to be the length of their vector difference then\"\n",
    "\n",
    "# Update tensors\n",
    "token_ids = tokenizer.tokenize_text(text)\n",
    "new_tensor_1, double_value = tokenizer.update_tensors(token_ids)\n",
    "\n",
    "# println(\"new_tensor_1:\", new_tensor_1)\n",
    "# println(\"double_value:\", double_value)\n",
    "\n",
    "id, sha1, hll_tensor = tokenizer.tensor_to_hlltensor(new_tensor_1)\n",
    "println(\"HLLSet:\", id, \"; \", sha1, \"; \", hll_tensor)\n",
    "\n",
    "hll_vector = tensor_to_array(hll_tensor)\n",
    "\n",
    "# println(\"HLLSet (Vector{Int64}):\", hll_vector)\n",
    "\n",
    "tensor_slice = tokenizer.hlltensor_to_tensor(hll_tensor)\n",
    "println(\"Tensor Slice:\", tensor_slice)\n",
    "\n",
    "# tokenizer.print_tensor_1(tensor=tokenizer.tensor_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.print_tensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个阶段，我们已经成功地将文本集成到大型语言模型（LLM）和元数据模型（MM）中，使得这两个模型中的令牌可以进行搜索和检索。\n",
    "\n",
    "为了从MM中检索与查询相关的所有HllSets，我们遵循以下步骤：\n",
    "\n",
    "1. 我们根据查询文本创建一个HllSet，使用我们之前采用的相同分词过程。\n",
    "2. 然后，我们在MM中搜索与查询HllSet相似的所有HllSets，使用余弦相似度来识别满足指定阈值的HllSets。\n",
    "3. 最后，利用令牌哈希到令牌ID的映射，我们用获取的令牌ID查询LLM。结果令牌将代表一组相关的令牌。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform search\n",
    "query = \"When the distance between two unit-length vectors is defined\"\n",
    "\n",
    "threshold = 0.1\n",
    "related_hllsets = pycall(tokenizer.search, PyObject, query, threshold)\n",
    "println(\"Related HLL sets: \", related_hllsets)\n",
    "\n",
    "# Get related tokens\n",
    "related_tokens = pycall(tokenizer.get_related_tokens, PyObject, related_hllsets)\n",
    "# println(\"Related tokens: \", related_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们现在可以请求LLM提供与提取的相关令牌一致的建议。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate meaningful text\n",
    "suggestions = []\n",
    "try    \n",
    "    suggestions = pycall(tokenizer.generate_text, PyObject, related_tokens, 3)\n",
    "catch e\n",
    "    println(\"Error generating text suggestions: \", e)\n",
    "end\n",
    "\n",
    "\n",
    "println(tokenizer.format_generated_texts(suggestions))\n",
    "\n",
    "# tokenizer.print_tensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们假设元模型中的HllSets以某种方式组织。一个常见的组织方法是使用余弦相似度等技术将HllSets分组到相关社区中。这使我们能够评估生成的建议，并根据我们的兴趣选择最相关的建议。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate generated texts\n",
    "communities = [\n",
    "    \n",
    "    \"4da9de3e80bcb65ee6169a411b0206ce45ba68bc\"\n",
    "    ]  # Load or define your communities of HLL sets here\n",
    "\n",
    "evaluation_results = pycall(tokenizer.evaluate_generated_texts, PyObject, suggestions, communities)\n",
    "println(\"Evaluation results: \", tokenizer.format_generated_texts(evaluation_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
